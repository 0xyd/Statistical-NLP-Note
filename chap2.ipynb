{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "The entroy is a the amount of information in a random variable.\n",
    "\n",
    "Let $p(x)$ be the mass function of a random variable X over a discrete set:\n",
    "$$p(x) = p(X = x), x \\in X$$\n",
    "\n",
    "Then we define the entropy as below:\n",
    "\n",
    "$$H(p) = H(X) = - \\sum_{x \\in X} p(x) \\log_2 p(x)$$\n",
    "\n",
    "Because the entropy is designed originally to measure the information loss, the base of the log is 2. \n",
    "\n",
    "Another way to interpret entropy is the expectation value of amount of information:\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "        H(X) & = - \\sum_{x \\in X} p(x) \\log_2 p(x) \\\\\n",
    "        & = \\sum_{x \\in X} p(x) \\log_2 \\frac{1}{p(x)} \\\\\n",
    "        & = E[\\log_2 \\frac{1}{p(x)}]\n",
    "    \\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Suppose there are four letters with different probabilities: $P(A) = \\frac{1}{2}$, $P(B) = \\frac{1}{8}$, $P(C) = \\frac{1}{4}$ and $P(D) = \\frac{1}{8}$, their entropy is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X) &= E[\\log_2 \\frac{1}{p(x)}] = \\sum_{x \\in A,B,C,D} p(x) \\log_2 \\frac{1}{p(x)} \\\\\n",
    "&= \\frac{1}{2} \\log_2 2 + \\frac{1}{8} \\log_2 8 + \\frac{1}{4} \\log_2 4 + \\frac{1}{8} \\log_2 8 = \\\\\n",
    "& = \\frac{1}{2} 1 + \\frac{1}{8} 3 + \\frac{1}{4} 2 + \\frac{1}{8} 3 \\\\\n",
    "& = 1 \\frac{3}{4}\\ (bits)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we can make a code takes $1 \\frac{3}{4}$ bits to transmit a letter. For instace, $A: 1$, $B: 000$, $C: 01$ and $D: 100$. \n",
    "\n",
    "Note that $H(X) \\geq 0$ and 0 only establishes when random variable X is no longer a probabilistic event. Besides, the longer the message is, the more entropy it has. For example, if we want to transmit 2 letters with the previous encoding scheme, we need average $2 \\times 1\\frac{3}{4} = 3.5$ bits for transmission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Entropy and Conditional Entropy\n",
    "\n",
    "### a) Joint Entropy\n",
    "\n",
    "The joint entropy of a pair of random variables $X, Y$ ~ $p(x, y)$ calculates amount of information of the associated events. It is defined as below:\n",
    "\n",
    "$$H(X, Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log p(x, y)$$\n",
    "\n",
    "### b) Conditional Entropy\n",
    "\n",
    "The conditonal entropy quantifies the amount of information for a random variable $Y$ under the random variable $X$ where $X, Y$ ~ $p(x, y)$.\n",
    "\n",
    "To simplify the derivation, we first consider the case when $X = x^*$, the entropy of $Y$ given $X = x^*$ is written as:\n",
    "$$H(Y|X=x^*) = - \\sum_{y \\in Y} p(y | X = x^*) log\\ p(y | X = x^*)$$\n",
    "\n",
    "The $H(Y|X)$ is the averaging entropy of $H(Y|X=x)$ for all $x \\in X$:\n",
    "$ $$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(Y|X) &= - \\sum_{x \\in X} p(x) H(Y | X = x) \\\\\n",
    "& = - \\sum_{x \\in X} p(x) \\sum_{y \\in Y} p(y | X = x) log\\ p(y | X = x) \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x) p(y | X = x) log\\ p(y | X = x) \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log\\ p(y | x) \\\\\n",
    "& = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log\\ \\frac{p(x, y)}{p(x)} \\\\\n",
    "& = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log\\ \\frac{p(x)}{p(x,y)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### c) Chain Rule\n",
    "\n",
    "So what is the relationship between the joint entroy and the conditional entropy? Let's start the joint into in expectation form:\n",
    "\n",
    "$$H(X, Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) log p(x, y) = - E_{p(x,y)} \\log p(x, y)$$\n",
    "\n",
    "Then, we substitute the joint probability with conditional probability:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X, Y) &= - E_{p(x,y)} \\log p(x, y) = - E_{p(x,y)} \\log p(y | x) p(x) \\\\\n",
    "&= - E_{p(x,y)} [\\log p(x) + \\log p(y | x)] \\\\\n",
    "&= - E_{p(x,y)} \\log p(x) - E_{p(x,y)} \\log p(y | x) \\\\\n",
    "&= H(X) + H(Y|X)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We have known that $H(X, Y) = H(X) + H(Y|X)$. We can now expand this form:\n",
    "$$H(X_1, X_2, X_3) = H(X_1, X_2) + H(X_3 | X_1, X_2)= H(X_1) + H(X_2 | X_1) + H(X_3 | X_1, X_2)$$\n",
    "\n",
    "To make it more general:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X_1, X_2, ... , X_n) &= H(X_1, X_2) + H(X_2 | X_1) + ... + H(X_n | X_1, .. X_{n-1}) \\\\\n",
    "&= \\sum^{n}_{i=1} H(X_i|X_{i-1}, .. X_1)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
